{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "\n",
    "class Prior():\n",
    "    def __init__(self, alpha1, beta1, alpha2, beta2):\n",
    "        self.alpha1 = alpha1\n",
    "        self.beta1 = beta1\n",
    "        self.alpha2 = alpha2\n",
    "        self.beta2 = beta2\n",
    "        \n",
    "class Net():\n",
    "    def __init__(self, net_type, nin, nhidden, nout, nwts, outfunc, \n",
    "                 alpha1=None, beta1=None, alpha2=None, beta2=None, w1=None, b1=None, w2=None, b2=None):\n",
    "        self.net_type = net_type\n",
    "        self.nin = nin\n",
    "        self.nhidden = nhidden\n",
    "        self.nout = nout\n",
    "        self.nwts = nwts\n",
    "        self.alpha1 = alpha1\n",
    "        self.beta1 = beta1\n",
    "        self.alpha2 = alpha2\n",
    "        self.beta2 = beta2\n",
    "        self.w1 = w1\n",
    "        self.b1 = b1\n",
    "        self.w2 = w2\n",
    "        self.b2 = b2        \n",
    "        outfns = ['linear', 'logistic', 'softmax']\n",
    "        if outfunc in outfns:\n",
    "            self.outfunc = outfunc\n",
    "        else:\n",
    "            raise ValueError('Undefined output function. Exiting.')\n",
    "\n",
    "def MLP(nin, nhidden, nout, outfunc, prior):\n",
    "    net_type = 'mlp'\n",
    "    nwts = (nin+1)*nhidden + (nhidden+1)*nout\n",
    "    net = Net(net_type, nin, nhidden, nout, nwts, outfunc)\n",
    "    net.alpha1 = prior.alpha1\n",
    "    net.beta1 = prior.beta1\n",
    "    net.alpha2 = prior.alpha2\n",
    "    net.beta2 = prior.beta2\n",
    "    net.w1  = prior.alpha1 * np.random.randn(nin, nhidden)\n",
    "    net.b1 = prior.beta1 * np.random.randn(1, nhidden)\n",
    "    net.w2 = prior.alpha2 * np.random.randn(nhidden, nout)\n",
    "    net.b2 = prior.beta2 * np.random.randn(1, nout)\n",
    "    return net\n",
    "    \n",
    "    \n",
    "def MLP_fwd(net, xvals_t):\n",
    "    ndata = xvals_t.shape[0]\n",
    "    z = np.tanh(xvals_t.reshape(-1, 1).dot(net.w1) + np.ones((ndata, 1)).dot(net.b1))\n",
    "    a = z.dot(net.w2) + np.ones((ndata, 1)).dot(net.b2)\n",
    "    \n",
    "    if net.outfunc == 'linear':\n",
    "        y = a\n",
    "    elif net.outfunc == 'logistic':\n",
    "        maxcut = -np.log(np.finfo(float).eps)\n",
    "        mincut = -np.log(1/np.finfo(float).tiny-1)\n",
    "        a = min(a, maxcut)\n",
    "        a = max(a, mincut)\n",
    "        y = 1/(1 + np.exp(-a))\n",
    "    elif net.outfunc == 'softmax':\n",
    "        maxcut = np.log(float('inf'))-np.log(net.nout)\n",
    "        mincut = np.log(np.finfo(float).tiny)\n",
    "        a = min(a, maxcut)\n",
    "        a = max(a, mincut)\n",
    "        temp = np.exp(a)\n",
    "        y = temp/(np.sum(temp, 1).dot(np.ones(1, net.nout)))\n",
    "    else:\n",
    "        raise ValueError('Unknown activation function')\n",
    "        \n",
    "    return y, a, z\n",
    "\n",
    "\n",
    "params0 = np.array([5, 1, 1, 1])\n",
    "params = np.tile(params0, (5, 1))\n",
    "sf = 5\n",
    "\n",
    "params[1, 0] = params0[0] * sf\n",
    "params[2, 1] = params0[1] * sf\n",
    "params[3, 2] = params0[2] * sf\n",
    "params[4, 3] = params0[3] * sf\n",
    "\n",
    "ntrials = 4\n",
    "\n",
    "for t in range(ntrials):\n",
    "    alpha1 = params[t, 0]\n",
    "    alpha2 = params[t, 2]\n",
    "    beta1 = params[t, 1]\n",
    "    beta2 = params[t, 3]\n",
    "    \n",
    "    nhidden = 12\n",
    "    nout = 1\n",
    "    prior = Prior(alpha1, beta1, alpha2, beta2)\n",
    "    xvals = np.arange(-1, 1.005, 0.005)\n",
    "    nsample = 10\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    for i in range(nsample):\n",
    "        net = MLP(1, nhidden, 1, 'linear', prior)\n",
    "        yvals, _, _ = MLP_fwd(net, xvals.T)\n",
    "        plt.plot(xvals.T, yvals, color='k', lw=2)\n",
    "        plt.ylim([-10, 10])\n",
    "        ttl = r'$\\alpha_1 = {},\\; \\beta_1 = {},\\; \\alpha_2 = {},\\; \\beta_2 = {}$'.format(\n",
    "            alpha1, beta1, alpha2, beta2)\n",
    "        plt.title(ttl, fontsize=18)\n",
    "    plt.show()\n",
    "    pml.save_fig(f'mlpPriors-{t}.pdf', dpi=300)\n",
    "        \n",
    "                 "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
