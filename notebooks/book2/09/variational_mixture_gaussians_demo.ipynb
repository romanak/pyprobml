{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a variational mixture of Gaussians.\n",
    "# For a matlab version, see  https://github.com/probml/pmtk3/blob/master/demos/mixGaussVbDemoFaithful.m\n",
    "# To run in colab, consider the following two lines\n",
    "# > pip uninstall tensorflow -y -q\n",
    "# > pip install -Uq tfp-nightly[jax] > /dev/null\n",
    "# Author: Gerardo Duran-Martin (@gerdm)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import variational_mixture_gaussians as vmg\n",
    "except ModuleNotFoundError:\n",
    "    %pip install variational_mixture_gaussians\n",
    "    import variational_mixture_gaussians as vmg\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "from scipy import stats as scistats\n",
    "from jax import random\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/probml/probml-data/main/data/faithful.txt'\n",
    "response = requests.get(url)\n",
    "rawdata = BytesIO(response.content)\n",
    "\n",
    "def plot_mixtures(X, r, mu, pi, Sigma, ax, step=0.01, cmap=\"viridis\", levels=1):\n",
    "    colors = [\"tab:red\", \"tab:blue\", \"tab:green\",\"tab:cyan\", \"tab:orange\", \"tab:purple\"]\n",
    "    x0, y0 = X.min(axis=0)\n",
    "    x1, y1 = X.max(axis=0)\n",
    "    xx, yy = np.mgrid[x0:x1:step, y0:y1:step]\n",
    "    zdom = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Nk = r.sum(axis=0)\n",
    "    \n",
    "    # Plotting distributions whose effective number of\n",
    "    # observations is at least 1\n",
    "    Norms = [scistats.multivariate_normal(mean=mui, cov=Sigmai)\n",
    "            for mui, Sigmai, N in zip(mu, Sigma, Nk) if N > 1]\n",
    "    \n",
    "    for Norm, color in zip(Norms, colors):\n",
    "        density = Norm.pdf(zdom).reshape(xx.shape)\n",
    "        ax.contour(xx, yy, density, levels=levels,\n",
    "            colors=color, linewidths=3)\n",
    "\n",
    "    ax.scatter(*X.T, alpha=0.7, c=r.argmax(axis=1), cmap=cmap)\n",
    "    plt.xlim(x0, x1)\n",
    "    plt.ylim(y0, y1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plt.rcParams[\"axes.spines.right\"] = False\n",
    "    plt.rcParams[\"axes.spines.top\"] = False\n",
    "\n",
    "    data = jnp.array(np.loadtxt(rawdata))\n",
    "    X = (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "    # Prior parameters\n",
    "    key = random.PRNGKey(3141)\n",
    "    N, M = X.shape\n",
    "    K = 6 \n",
    "\n",
    "    m_0 = X.mean(axis=0, keepdims=True).T * jnp.ones((M, K)) \n",
    "    m_0 = m_0 + random.normal(key, (M, K))\n",
    "\n",
    "    beta_0 = jnp.ones(K)\n",
    "    alpha_0 = jnp.ones(K)  * 0.001\n",
    "    eta_0 =  4 * jnp.ones(K)\n",
    "\n",
    "    W_0 = jnp.eye(M)[None, ...] * jnp.ones((K, 1, 1)) / 5\n",
    "\n",
    "    vbmixture = vmg.VBMixture()\n",
    "    n_iterations = 100\n",
    "    hist = vbmixture.fit(X, m_0, W_0, beta_0, alpha_0, eta_0, n_iterations, store_hist=True)\n",
    "    iterations_range = np.arange(n_iterations)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(iterations_range[2:], vbmixture.lower_bound_hist[2:])\n",
    "    ax.scatter(iterations_range[2::10], vbmixture.lower_bound_hist[2::10])\n",
    "    ax.set_title(\"variational Bayes objective for GMM on old faithful data\")\n",
    "    ax.set_ylabel(\"Lower bound on log marginal likelihood\")\n",
    "    pml.savefig(\"gmmvb-lower-bound.pdf\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    it1 = hist[1]\n",
    "    pi_k, mu_k, Sigma_k = vbmixture.expected_values(it1)\n",
    "    r_nk = vbmixture.compute_responsibilities(X, it1[\"alpha\"],it1[\"beta\"],it1[\"eta\"],it1[\"m\"],it1[\"W\"])\n",
    "    plot_mixtures(X, r_nk, mu_k, pi_k, Sigma_k, ax, levels=1)\n",
    "    ax.set_title(f\"Iter 1\")\n",
    "    pml.savefig(\"gmmvb-cluster-it-1.pdf\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    pi_k, mu_k, Sigma_k = vbmixture.expected_values()\n",
    "    plot_mixtures(X, vbmixture.r_nk, mu_k, pi_k, Sigma_k, ax, levels=1)\n",
    "    ax.set_title(f\"Iter {n_iterations}\")\n",
    "    pml.savefig(f\"gmmvb-cluster-it-{n_iterations}.pdf\")\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(np.arange(K), it1[\"alpha\"])\n",
    "    ax.set_xticks(range(K))\n",
    "    ax.set_xticklabels([r\"$\\alpha_\"f\"{k+1}$\" for k in range(K)], fontsize=13);\n",
    "    ax.set_title(\"iter 1\") \n",
    "    pml.savefig(\"gmmvb-alpha-dist-1.pdf\")\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(np.arange(K), vbmixture.alpha_k)\n",
    "    ax.set_xticks(range(K))\n",
    "    ax.set_xticklabels([r\"$\\alpha_\"f\"{k+1}$\" for k in range(K)], fontsize=13)\n",
    "    ax.set_title(\"Iter 100\")\n",
    "    pml.savefig(f\"gmmvb-alpha-dist-{n_iterations}.pdf\")\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
