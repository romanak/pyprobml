{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b7223a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "from matplotlib import pyplot as plt\n",
    "from cycler import cycler\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats.norm as jnorm\n",
    "from jax import grad\n",
    "try:\n",
    "    import probml_utils as pml\n",
    "except ModuleNotFoundError:\n",
    "    %pip install git+https://github.com/probml/probml-utils.git\n",
    "    import probml_utils as pml\n",
    "try:\n",
    "    from statsmodels.discrete.discrete_model import Probit\n",
    "except ModuleNotFoundError:\n",
    "    %pip install statsmodels\n",
    "    from statsmodels.discrete.discrete_model import Probit\n",
    "\n",
    "cb_color = ['#377eb8', '#ff7f00']\n",
    "\n",
    "cb_cycler = (cycler(linestyle=['-', '--', '-.']) * cycler(color=cb_color))\n",
    "plt.rc('axes', prop_cycle=cb_cycler)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class ProbitReg:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loglikehist = []\n",
    "        self.max_iter = 100\n",
    "        self.tolerance = 1e-4\n",
    "        self.w = []\n",
    "\n",
    "    # Probit-loss = (1-y)*log(1-gauss.cdf(X.w)) - (1-y)*log(gauss.cdf(-(X.w))\n",
    "    def probitloss(self, X, y, w):  # NLL\n",
    "        return -jnp.sum(y * jnorm.logcdf(jnp.dot(X, w))) - \\\n",
    "               jnp.sum((1 - y) * jnorm.logcdf(-jnp.dot(X, w)))\n",
    "\n",
    "    def objfn(self, X, y, w, lam):  # penalized likelihood.\n",
    "        return jnp.sum(lam * jnp.square(w[1:])) - self.probitloss(X, y, w)\n",
    "\n",
    "    def probreg_fit_em(self, X, y, lam):\n",
    "\n",
    "        self.w = np.linalg.lstsq(\n",
    "            X + np.random.rand(X.shape[0], X.shape[1]), y, rcond=None)[0].reshape(-1, 1)\n",
    "\n",
    "        def estep(w):\n",
    "            u = X @ w\n",
    "            z = u + norm.pdf(u) / ((y == 1) - norm.cdf(-u))\n",
    "            loglik = self.objfn(X, y, w, lam)\n",
    "            return z, loglik\n",
    "\n",
    "        # M step function is the ridge regression\n",
    "        def mstep(X, y, lam):\n",
    "            return ridge_reg(X, y, lam)\n",
    "\n",
    "        i = 1\n",
    "        stop = False\n",
    "        while not stop:\n",
    "            z, loglike = estep(self.w)\n",
    "            self.loglikehist.append(loglike)\n",
    "            self.w = mstep(X, z, lam)\n",
    "            if i >= self.max_iter:\n",
    "                stop = True\n",
    "            elif i > 1:\n",
    "                # if slope becomes less than tolerance.\n",
    "                stop = np.abs((self.loglikehist[i - 1] - self.loglikehist[i - 2]) / (\n",
    "                        self.loglikehist[i - 1] + self.loglikehist[i - 2])) <= self.tolerance / 2\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        self.loglikehist = self.loglikehist[0:i - 1]\n",
    "\n",
    "        return self.w, np.array(self.loglikehist)\n",
    "\n",
    "    def probit_reg_fit_gradient(self, X, y, lam):\n",
    "        winit = jnp.linalg.lstsq(\n",
    "            X + np.random.rand(X.shape[0], X.shape[1]), y, rcond=None)[0].reshape(-1, 1)\n",
    "\n",
    "        self.loglikehist = []\n",
    "\n",
    "        self.loglikehist.append((-self.objfn(X, y, winit, lam)))\n",
    "\n",
    "        def obj(w):\n",
    "            w = w.reshape(-1, 1)\n",
    "            # PNLL\n",
    "            return self.probitloss(X, y, w) + jnp.sum(lam * jnp.square(w[1:]))\n",
    "\n",
    "        def grad_obj(w):\n",
    "            return grad(obj)(w)\n",
    "\n",
    "        def callback(w):\n",
    "            loglik = obj(w)  # LL\n",
    "\n",
    "            self.loglikehist.append(loglik)\n",
    "\n",
    "        res = minimize(\n",
    "            obj,\n",
    "            x0=winit,\n",
    "            jac=grad_obj,\n",
    "            callback=callback,\n",
    "            method='BFGS')\n",
    "        return res['x'], np.array(self.loglikehist[0:-1])\n",
    "\n",
    "    def predict(self, X, w):\n",
    "        p = jnorm.cdf(jnp.dot(X, w))\n",
    "        y = np.array((p > 0.5), dtype='int32')\n",
    "        return y, p\n",
    "\n",
    "\n",
    "# using matrix inversion for ridge regression\n",
    "def ridge_reg(X, y, lambd):  # returns weight vectors.\n",
    "    D = X.shape[1]\n",
    "    w = np.linalg.inv(X.T @ X + lambd * np.eye(D, D)) @ X.T @ y\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def flip_bits(y, p):\n",
    "    x = np.random.rand(y.shape[0], 1) < p\n",
    "    y[x < p] = 1 - y[x < p]\n",
    "    return y\n",
    "\n",
    "\n",
    "n, d = 100, 2\n",
    "data_x = np.random.randn(n, d)\n",
    "w = np.random.randn(d, 1)\n",
    "data_y = flip_bits((data_x @ w > 0), 0)\n",
    "\n",
    "lam = 1e-2\n",
    "\n",
    "# statsmodel.Probit\n",
    "sm_probit_reg = Probit(exog=data_x, endog=data_y).fit(disp=0, method='bfgs')\n",
    "sm_probit_prob = sm_probit_reg.predict(exog=data_x)\n",
    "\n",
    "# Our Implementation:\n",
    "probit_reg = ProbitReg()\n",
    "\n",
    "# EM:\n",
    "em_w, obj_trace_em = probit_reg.probreg_fit_em(data_x, data_y, lam)\n",
    "em_ypred, em_prob = probit_reg.predict(data_x, em_w)\n",
    "\n",
    "# gradient:\n",
    "gradient_w, obj_trace_gradient = probit_reg.probit_reg_fit_gradient(\n",
    "    data_x, data_y, lam)\n",
    "gradient_ypred, gradient_prob = probit_reg.predict(data_x, gradient_w)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(sm_probit_prob, em_prob, 'o')\n",
    "plt.xlabel('statsmodel.probit')\n",
    "plt.ylabel('em')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(gradient_prob, em_prob, 'o')\n",
    "plt.xlabel('bfgs')\n",
    "plt.ylabel('em')\n",
    "plt.title('probit regression with L2 regularizer of {0:.3f}'.format(lam))\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(-obj_trace_em.flatten(), '-o', linewidth=2)\n",
    "plt.plot(obj_trace_gradient.flatten(), ':s', linewidth=1)\n",
    "plt.legend(['em', 'bfgs'])\n",
    "plt.title('probit regression with L2 regularizer of {0:.3f}'.format(lam))\n",
    "plt.ylabel('logpost')\n",
    "plt.xlabel('iter')\n",
    "pml.save_fig('../figures/probitRegDemoNLL.pdf')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
